{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE_CIFAR10.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7azs6Wcejjd_","colab_type":"code","colab":{}},"source":["# Block1\n","import chainer\n","chainer.print_runtime_info()\n","print('GPU availability:', chainer.cuda.available)\n","print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXl4J6YEF3yO","colab_type":"code","colab":{}},"source":["# Block2\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxFkxdnxCgXY","colab_type":"code","colab":{}},"source":["# Block3\n","# Network model_________________________________________________________________\n","\n","import math\n","import numpy as np\n","\n","import chainer\n","import chainer.distributions as D\n","import chainer.functions as F\n","import chainer.links as L\n","from chainer import reporter\n","    \n","# For stabilization and increasing training speed\n","# But, in this program we don't use these functions\n","def feature_vector_normalization(x, eps=1e-8):\n","    # x: (B, C, H, W)\n","    alpha = 1.0 / F.sqrt(F.mean(x*x, axis=1, keepdims=True) + eps)\n","    return F.broadcast_to(alpha, x.data.shape) * x\n","\n","def minibatch_std(x):\n","    m = F.mean(x, axis=0, keepdims=True)\n","    v = F.mean((x - F.broadcast_to(m, x.shape))*(x - F.broadcast_to(m, x.shape)), axis=0, keepdims=True)\n","    sd = F.mean(F.sqrt(v + 1e-8), keepdims=True)\n","    std = F.broadcast_to(std, (x.shape[0], 1, x.shape[2], x.shape[3]))\n","    return F.concat([x, std], axis=1)\n","    \n","class EqualizedConv2d(chainer.Chain):\n","    def __init__(self, in_ch, out_ch, ksize, stride, pad):\n","        w = chainer.initializers.Normal(1.0) # equalized learning rate\n","        self.inv_c = np.sqrt(2.0/(in_ch*ksize**2))\n","        super(EqualizedConv2d, self).__init__()\n","        with self.init_scope():\n","            self.c = L.Convolution2D(in_ch, out_ch, ksize, stride, pad, initialW=w)\n","    def __call__(self, x):\n","        return self.c(self.inv_c * x)\n","    \n","class EqualizedLinear(chainer.Chain):\n","    def __init__(self, in_ch, out_ch):\n","        w = chainer.initializers.Normal(1.0) # equalized learning rate\n","        self.inv_c = np.sqrt(2.0/in_ch)\n","        super(EqualizedLinear, self).__init__()\n","        with self.init_scope():\n","            self.c = L.Linear(in_ch, out_ch, initialW=w)\n","    def __call__(self, x):\n","        return self.c(self.inv_c * x)\n","    \n","\n","# VAE___________________________________________________________________________\n","class AvgELBOLoss(chainer.Chain):\n","    \"\"\"gLoss function of VAE.\n","    The loss value is equal to ELBO (Evidence Lower Bound)\n","    multiplied by -1.\n","    Args:\n","        encoder (chainer.Chain): A neural network which outputs variational\n","            posterior distribution q(z|x) of a latent variable z given\n","            an observed variable x.\n","        decoder (chainer.Chain): A neural network which outputs conditional\n","            distribution p(x|z) of the observed variable x given\n","            the latent variable z.\n","        prior (chainer.Chain): A prior distribution over the latent variable z.\n","        beta (float): Usually this is 1.0. Can be changed to control the\n","            second term of ELBO bound, which works as regularization.\n","        k (int): Number of Monte Carlo samples used in encoded vector.\n","    \"\"\"\n","\n","    def __init__(self, encoder, decoder, prior, batch_size, beta=1.0, k=1):\n","        super(AvgELBOLoss, self).__init__()\n","        self.beta = beta\n","        self.k = k\n","        self.batch_size = batch_size\n","\n","        with self.init_scope():\n","            self.encoder = encoder\n","            self.decoder = decoder\n","            self.prior = prior\n","\n","    def __call__(self, x1, x2):\n","        q_z = self.encoder(x1)\n","        z = q_z.sample(self.k)\n","        p_x = self.decoder(z)\n","        p_z = self.prior()\n","        p_x = p_x * 255\n","\n","        reconstr = 0\n","        for i in range(self.k):\n","            reconstr += F.mean_squared_error(x2, p_x) / self.k\n","            \n","        kl_penalty = F.mean(F.sum(chainer.kl_divergence(q_z, p_z), axis=-1))\n","        loss = reconstr + self.beta * kl_penalty\n","        reporter.report({'loss': loss}, self)\n","        reporter.report({'reconstr': reconstr}, self)\n","        reporter.report({'kl_penalty': kl_penalty}, self)\n","        return loss\n","\n","\n","class Encoder(chainer.Chain):\n","\n","    def __init__(self, n_latent, ch=512, bottom_width=4, wscale=0.02):\n","        super(Encoder, self).__init__()\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)\n","            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)\n","            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)\n","            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)\n","            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)\n","            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)\n","            self.mu = L.Linear(\n","                 bottom_width * bottom_width * ch, n_latent, initialW=w)\n","            self.ln_sigma = L.Linear(\n","                 bottom_width * bottom_width * ch, n_latent, initialW=w)\n","            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)\n","            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)\n","            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)\n","            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)\n","\n","    def forward(self, x):\n","        h = F.leaky_relu(self.c0_0(x))\n","        h = F.leaky_relu(self.bn0_1(self.c0_1(h)))\n","        h = F.leaky_relu(self.bn1_0(self.c1_0(h)))\n","        h = F.leaky_relu(self.bn1_1(self.c1_1(h)))\n","        h = F.leaky_relu(self.bn2_0(self.c2_0(h)))\n","        h = F.leaky_relu(self.bn2_1(self.c2_1(h)))\n","        h = F.leaky_relu(self.bn3_0(self.c3_0(h)))\n","        mu = self.mu(h)\n","        ln_sigma = self.ln_sigma(h)  # log(sigma)\n","        return D.Normal(loc=mu, log_scale=ln_sigma)\n","\n","\n","class Decoder(chainer.Chain):\n","\n","    def __init__(self, n_latent, bottom_width=4, ch=512, wscale=0.02):\n","        super(Decoder, self).__init__()\n","        self.ch = ch\n","        self.bottom_width = bottom_width\n","        with self.init_scope():\n","            w = chainer.initializers.Normal(wscale)\n","            self.l0 = L.Linear(\n","                n_latent, bottom_width * bottom_width * ch, initialW=w)\n","            self.c1 = L.Convolution2D(ch, ch // 2, 3, 1, 1, initialW=w)\n","            self.c2 = L.Convolution2D(ch // 2, ch // 4, 3, 1, 1, initialW=w)\n","            self.c3 = L.Convolution2D(ch // 4, ch // 8, 3, 1, 1, initialW=w)\n","            self.c4 = L.Convolution2D(ch // 8, 3, 3, 1, 1, initialW=w)\n","            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)\n","            self.bn1 = L.BatchNormalization(ch // 2)\n","            self.bn2 = L.BatchNormalization(ch // 4)\n","            self.bn3 = L.BatchNormalization(ch // 8)\n","\n","    def forward(self, z, inference=False):\n","        h = F.reshape(F.tanh(self.bn0(self.l0(z[0]))), \n","                      (len(z[0]), self.ch, self.bottom_width, self.bottom_width))\n","        h = F.leaky_relu(self.bn1(self.c1(h)))\n","        h = F.unpooling_2d(h, 2, 2, 0, outsize=(8, 8))\n","        h = F.leaky_relu(self.bn2(self.c2(h)))\n","        h = F.unpooling_2d(h, 2, 2, 0, outsize=(16, 16))\n","        h = F.leaky_relu(self.bn3(self.c3(h)))\n","        h = F.unpooling_2d(h, 2, 2, 0, outsize=(32 ,32))\n","        x = F.sigmoid(self.c4(h))\n","        return x\n","\n","\n","class Prior(chainer.Link):\n","\n","    def __init__(self, n_latent):\n","        super(Prior, self).__init__()\n","\n","        self.loc = np.zeros(n_latent, np.float32)\n","        self.scale = np.ones(n_latent, np.float32)\n","        self.register_persistent('loc')\n","        self.register_persistent('scale')\n","\n","    def forward(self):\n","        return D.Normal(self.loc, scale=self.scale)\n","\n","\n","def make_encoder(n_latent):\n","    return Encoder(n_latent)\n","\n","\n","def make_decoder(n_latent):\n","    return Decoder(n_latent)\n","\n","\n","def make_prior(n_latent):\n","    return Prior(n_latent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBT4ExrkM3QM","colab_type":"code","colab":{}},"source":["# Block4\n","# train_________________________________________________________________________\n","\"\"\"Chainer example: train a VAE on MNIST\n","\"\"\"\n","import argparse\n","import os\n","import glob\n","from PIL import Image\n","\n","import numpy as np\n","\n","import chainer\n","from chainer import training\n","from chainer.training import extensions\n","from chainer import serializers\n","\n","def main():\n","    \"\"\"\n","    parser = argparse.ArgumentParser(description='VAE_cnn for FDC')\n","    parser.add_argument('--initmodel', '-m', default='',\n","                        help='Initialize the model from given file')\n","    parser.add_argument('--resume', '-r', default='',\n","                        help='Resume the optimization from snapshot')\n","    parser.add_argument('--gpu', '-g', default=0, type=int,\n","                        help='GPU ID (negative value indicates CPU)')\n","    parser.add_argument('--out', '-o', default='results',\n","                        help='Directory to output the result')\n","    parser.add_argument('--epoch', '-e', default=100, type=int,\n","                        help='number of epochs to learn')\n","    parser.add_argument('--dim-z', '-z', default=20, type=int,\n","                        help='dimention of encoded vector')\n","    parser.add_argument('--dim-h', default=500, type=int,\n","                        help='dimention of hidden layer')\n","    parser.add_argument('--beta', default=1.0, type=float,\n","                        help='Regularization coefficient for '\n","                             'the second term of ELBO bound')\n","    parser.add_argument('--k', '-k', default=1, type=int,\n","                        help='Number of Monte Carlo samples used in '\n","                             'encoded vector')\n","    parser.add_argument('--binary', action='store_true',\n","                        help='Use binarized MNIST')\n","    parser.add_argument('--batch-size', '-b', type=int, default=100,\n","                        help='learning minibatch size')\n","    parser.add_argument('--test', action='store_true',\n","                        help='Use tiny datasets for quick tests')\n","    args = parser.parse_args()\n","    \"\"\"\n","    \n","    dim_z = 50\n","    batch_size = 1000\n","    epoch = 500\n","    beta = 1.0\n","    k = 1\n","    # Please change variable \"out\"\n","    out = \"\"\n","    snapshot_interval = 100\n","\n","    print('GPU: 0')\n","    print('# dim z: {}'.format(dim_z))\n","    print('# Minibatch-size: {}'.format(batch_size))\n","    print('# epoch: {}'.format(epoch))\n","    print('')\n","    \n","    # Prepare VAE model\n","    encoder = make_encoder(dim_z)\n","    decoder = make_decoder(dim_z)\n","    prior = make_prior(dim_z)\n","    avg_elbo_loss = AvgELBOLoss(encoder, decoder, prior, batch_size, beta=beta, k=k)\n","    \n","    avg_elbo_loss.to_gpu(0)\n","        \n","    # Setup an optimizer\n","    optimizer = chainer.optimizers.Adam()\n","    optimizer.setup(avg_elbo_loss)\n","    \n","    # Load CIFAR-10 dataset\n","    train, test = chainer.datasets.get_cifar10(withlabel=False, scale=255.)\n","    \n","    # If you use CIFAR-10, pleae remove \"shuffle=False\" in train_iter.\n","    train_iter = chainer.iterators.SerialIterator(\n","        train, batch_size, shuffle=False)\n","    valid_iter = chainer.iterators.SerialIterator(\n","        valid, len(valid), repeat=False, shuffle=False)\n","    test_iter = chainer.iterators.SerialIterator(\n","        test, len(test), repeat=False, shuffle=False)\n","    \n","    # Set up an updater. StandardUpdater can explicitly specify a loss function\n","    # used in the training with 'loss_func' option\n","    updater = training.updaters.StandardUpdater(\n","        train_iter, optimizer, device=0, loss_func=avg_elbo_loss)\n","    \n","    trainer = training.Trainer(updater, (epoch, \"epoch\"), out=out)\n","    snapshot_interval = (snapshot_interval, \"iteration\")\n","    trainer.extend(extensions.Evaluator(valid_iter, avg_elbo_loss, device=0))\n","    trainer.extend(extensions.PlotReport([\"main/reconstr\", \"validation/main/reconstr\"], x_key=\"epoch\", file_name=\"loss.png\"))\n","    trainer.extend(extensions.snapshot_object(\n","        encoder, \"encoder_{.updater.iteration}.npz\"), trigger=snapshot_interval)\n","    trainer.extend(extensions.snapshot_object(\n","        decoder, \"decoder_{.updater.iteration}.npz\"), trigger=snapshot_interval)\n","    trainer.extend(extensions.LogReport())\n","    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss',\n","                                           'main/reconstr', 'main/kl_penalty', 'elapsed_time']))\n","    trainer.extend(extensions.ProgressBar())\n","        \n","    # Run the training\n","    trainer.run()\n","    \n","    \n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[]}]}